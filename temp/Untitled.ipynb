{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fasttext\n",
    "import stringdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "global embeddings_model\n",
    "global embeddings_size\n",
    "global data_path\n",
    "\n",
    "data_path = \"../data/\"\n",
    "\n",
    "def __main__():\n",
    "    global embeddings_model\n",
    "    global embeddings_size\n",
    "    ## hyperparameters ##\n",
    "    embeddings_size= 50\n",
    "    text_transcript = data_path + 'text_transcript.txt'\n",
    "    ## main ##\n",
    "    train = pd.read_csv(data_path + 'friends.train.episode_delim.conll',sep='\\s+',header=None,comment='#')\n",
    "    train_scene = pd.read_csv(data_path + 'friends.train.scene_delim.conll',sep='\\s+',header=None,comment='#')\n",
    "    trial = pd.read_csv(data_path + 'friends.trial.episode_delim.conll',sep='\\s+',header=None,comment='#')\n",
    "    trial_scene = pd.read_csv(data_path + 'friends.trial.scene_delim.conll',sep='\\s+',header=None,comment='#')\n",
    "    dfs = [train,train_scene,trial,trial_scene]\n",
    "    # with open(text_transcript,'wt') as f:\n",
    "       # f.write(make_transcript(dfs))\n",
    "    # embeddings_model = fasttext.skipgram(text_transcript,'embeddings_model',min_count=1,dim=50)\n",
    "    embeddings_model = fasttext.load_model('embeddings_model.bin')\n",
    "    print(\"Embeddings trained\")\n",
    "    pairs = []\n",
    "    for df in dfs:                                                                                                                                                                                                                                                  \n",
    "        pairs.append(make_feature_matrices(train))\n",
    "    pairs = np.array(pairs)\n",
    "    np.save('pairs.npy',pairs)\n",
    "\n",
    "def make_feature_matrices(df):\n",
    "    mentions,mentions_y,mentions_idx = get_mention_arrays(df)\n",
    "    print('Mentions done')\n",
    "    words,words_idx = get_words_idx(df)\n",
    "    pre_words = get_pre_words(mentions_idx,words,words_idx)\n",
    "    next_words = get_next_words(mentions_idx,words,words_idx)\n",
    "    print('words done')\n",
    "    sents,sents_idx = get_sent_idx(df)\n",
    "    curr_sents = get_current_sents(mentions_idx,sents,sents_idx)\n",
    "    next_sents = get_next_sents(mentions_idx,sents,sents_idx)\n",
    "    pre_sents = get_pre_sents(mentions_idx,sents,sents_idx)\n",
    "    print('sents done')\n",
    "    ut,ut_idx = get_utterances_idx(df)\n",
    "    curr_ut = get_current_utterance(mentions_idx,ut,ut_idx)\n",
    "    next_ut = get_next_utterances(mentions_idx,ut,ut_idx)\n",
    "    pre_ut = get_pre_utterances(mentions_idx,ut,ut_idx)\n",
    "    print('ut done')\n",
    "    speakers,speakers_idx = get_utterances_idx(df)\n",
    "    curr_speaker = get_current_speaker(mentions_idx,speakers,speakers_idx)\n",
    "    pre_speakers = get_pre_speakers(mentions_idx,speakers,speakers_idx)\n",
    "    print('speakers done')\n",
    "    phi = [0]*5\n",
    "    phi[1] = get_phi_1(mentions)\n",
    "    phi[2] = get_phi_2(pre_words,next_words,mentions)\n",
    "    phi[3] = get_phi_3(pre_sents,next_sents,curr_sents)\n",
    "    phi[4] = get_phi_4(pre_ut,next_ut,curr_ut)\n",
    "    phi[1] = np.reshape(phi[1],[-1,3,embeddings_size,1])\n",
    "    phi[2] = np.reshape(phi[2],[-1,7,embeddings_size,1])\n",
    "    phi[3] = np.reshape(phi[3],[-1,5,embeddings_size,1])\n",
    "    phi[4] = np.reshape(phi[4],[-1,5,embeddings_size,1])\n",
    "    print('phi done')\n",
    "    pairs = make_mention_pairs(phi,mentions_y)\n",
    "    print('pairs done')\n",
    "    return pairs\n",
    "\n",
    "def make_mention_pairs(phi,mentions_y,window=7):\n",
    "    tuples = []\n",
    "    for i in range(len(phi[1])):\n",
    "        for j in range(i+1,window+i+1):\n",
    "            if j<len(phi[1]):\n",
    "                tuples.append([\\\n",
    "                              [phi[k][i] for k in range(1,5)],\\\n",
    "                              [phi[k][j] for k in range(1,5)],\\\n",
    "                              1 if mentions_y[i]==mentions_y[j] else 0\\\n",
    "                             ])\n",
    "            else:\n",
    "                break\n",
    "    return tuples\n",
    "    \n",
    "def get_phi_d(curr_speakers,pre_speakers):\n",
    "    curr_s = get_embeddings(curr_speakers)\n",
    "    pre_s = [get_embeddings(i) for i in pre_speakers]\n",
    "    return np.array([np.vstack((i,j[0],j[1])) for i,j in zip(curr_s,pre_s)])\n",
    "    \n",
    "def get_phi_1(mentions):\n",
    "    phi = []\n",
    "    for i in mentions:\n",
    "        t = i.split()\n",
    "        if len(t)==0:\n",
    "            raise ValueError('Empty mention')\n",
    "        elif len(t)==1:\n",
    "            phi.append(get_embeddings(t+['','']))\n",
    "        elif len(t)==2:\n",
    "            phi.append(get_embeddings(t+['']))\n",
    "        else:\n",
    "            phi.append(get_embeddings(t[0:3]))\n",
    "    return np.array(phi)\n",
    "\n",
    "def get_phi_2(pre_words,next_words,mentions):\n",
    "    pre = [get_embeddings(i) for i in pre_words]\n",
    "    suc = [get_embeddings(i) for i in next_words]\n",
    "    avg = get_avg_embeddings(mentions)\n",
    "    return np.array([np.vstack((i[0],i[1],i[2],j[0],j[1],j[2],k)) for i,j,k in zip(pre,suc,avg)])\n",
    "\n",
    "def get_phi_3(pre_sents,next_sents,curr_sents):\n",
    "    p = get_multiple_avg_embeddings(pre_sents)\n",
    "    s = get_avg_embeddings(next_sents)\n",
    "    c = get_avg_embeddings(curr_sents)\n",
    "    return np.array([np.vstack((i[0],i[1],i[2],j,k)) for i,j,k in zip(p,s,c)])\n",
    "\n",
    "def get_phi_4(pre_ut,next_ut,curr_ut):\n",
    "    p = get_multiple_avg_embeddings(pre_ut)\n",
    "    s = get_avg_embeddings(next_ut)\n",
    "    c = get_avg_embeddings(curr_ut)\n",
    "    return np.array([np.vstack((i[0],i[1],i[2],j,k)) for i,j,k in zip(p,s,c)])\n",
    "\n",
    "def make_transcript(dfs,idx=6):\n",
    "    words = ''\n",
    "    for df in dfs:\n",
    "        words += ' '.join(list(df[idx]))\n",
    "    return words\n",
    "\n",
    "def clean(df):\n",
    "    df = df[[0,2,3,4,5,6,9,10,11]]\n",
    "    return df\n",
    "\n",
    "def get_mention_rows(df):\n",
    "    return df[df[11]!='-'][df[11]!='NaN'][df[11]!=np.nan]\n",
    "\n",
    "def get_words_idx(df):\n",
    "    l = df[6].values\n",
    "    return l, range(len(l))\n",
    "\n",
    "def get_pre_words(mentions_idx,words,words_idx):\n",
    "    pre_words = []\n",
    "    for idx in mentions_idx:\n",
    "        pre_words.append([\\\n",
    "                           words[words_idx[idx[0]-1]] if (idx[-1]-1)>=0 else '',\\\n",
    "                           words[words_idx[idx[0]-2]] if (idx[-1]-2)>=0 else '',\\\n",
    "                           words[words_idx[idx[0]-3]] if (idx[-1]-3)>=0 else ''\\\n",
    "                          ])\n",
    "    return pre_words\n",
    "\n",
    "def get_next_words(mentions_idx,words,words_idx):\n",
    "    next_words = []\n",
    "    for idx in mentions_idx:\n",
    "        next_words.append([\\\n",
    "                           words[words_idx[idx[0]+1]] if (idx[-1]+1)<len(words) else '',\\\n",
    "                           words[words_idx[idx[0]+2]] if (idx[-1]+2)<len(words) else '',\\\n",
    "                           words[words_idx[idx[0]+3]] if (idx[-1]+3)<len(words) else ''\\\n",
    "                          ])\n",
    "    return next_words\n",
    "\n",
    "def get_sent_idx(df):\n",
    "    sents = []\n",
    "    sents_idx = []\n",
    "    cnt = 0\n",
    "    for row_no,row in df.iterrows():\n",
    "        if row[2]==0:\n",
    "            if row_no!=0:\n",
    "                cnt +=1\n",
    "                sents.append(sent_buf)\n",
    "            sent_buf = row[6]\n",
    "        else:\n",
    "            sent_buf += ' ' + row[6]\n",
    "        sents_idx.append(cnt)\n",
    "    if df.iloc[len(df)-1][2]!=0:\n",
    "        sents.append(sent_buf)\n",
    "        sents_idx.append(cnt)\n",
    "    return sents,sents_idx\n",
    "\n",
    "def get_utterances_idx(df):\n",
    "    ut = []\n",
    "    ut_idx = []\n",
    "    cnt = 0\n",
    "    speaker = None\n",
    "    for row_no,row in df.iterrows():\n",
    "        if row[9]!=speaker:\n",
    "            if row_no!=0:\n",
    "                ut.append(ut_buf)\n",
    "                cnt += 1\n",
    "            speaker = row[9]\n",
    "            ut_buf = row[6]\n",
    "        else:\n",
    "            ut_buf += ' ' + row[6]\n",
    "        ut_idx.append(cnt)\n",
    "    if df.iloc[len(df)-1][9]==speaker:\n",
    "        ut.append(ut_buf)\n",
    "        ut_idx.append(cnt)\n",
    "    return ut,ut_idx\n",
    "\n",
    "def get_speakers_idx(df):\n",
    "    ## change range ##\n",
    "    speaker = None\n",
    "    speakers = []\n",
    "    speakers_idx = []\n",
    "    cnt = -1\n",
    "    for row_no,row in df.iterrows():\n",
    "        if row[9]!=speaker:\n",
    "            speakers.append(row[9])\n",
    "            cnt += 1\n",
    "            speaker = row[9]\n",
    "        speakers_idx.append(cnt)\n",
    "    return speakers, speakers_idx\n",
    "\n",
    "def get_mention_arrays(df):\n",
    "    mentions = []\n",
    "    mentions_y = []\n",
    "    mentions_idx = []\n",
    "    mention_f = False\n",
    "    for row_no,row in df.iterrows():\n",
    "        if mention_f and row[11]=='-':\n",
    "            mention_buf += ' ' + row[6]\n",
    "        elif row[11]=='-' or type(row[11])==float:\n",
    "            continue\n",
    "        elif ('(' in row[11]) and (')' in row[11]):\n",
    "            mentions.append(row[6])\n",
    "            mentions_y.append(int(row[11][1:-1]))\n",
    "            mentions_idx.append([row_no,row_no])\n",
    "        elif ('(' in row[11]):\n",
    "            mention_f = True\n",
    "            mention_buf = row[6]\n",
    "            mention_idx_buf = row_no\n",
    "        elif (')' in row[11]):\n",
    "            mention_f = False\n",
    "            mention_buf += ' ' + row[6]\n",
    "            mentions.append(mention_buf)\n",
    "            mentions_y.append(int(row[11][:-1]))\n",
    "            mentions_idx.append([mention_idx_buf,row_no])\n",
    "    return mentions,mentions_y,mentions_idx\n",
    "\n",
    "def get_current_sents(mentions_idx,sents,sents_idx):\n",
    "    curr_sents = []\n",
    "    for idx in mentions_idx:\n",
    "        curr_sents.append(sents[sents_idx[idx[0]]])\n",
    "    return curr_sents\n",
    "\n",
    "def get_next_sents(mentions_idx,sents,sents_idx):\n",
    "    next_sents = []\n",
    "    for idx in mentions_idx:\n",
    "        t = sents_idx[idx[-1]]+1\n",
    "        if t<len(sents):\n",
    "            next_sents.append(sents[t])\n",
    "    return next_sents\n",
    "\n",
    "def get_pre_sents(mentions_idx,sents,sents_idx):\n",
    "    pre_sents = []\n",
    "    for idx in mentions_idx:\n",
    "        t = sents_idx[idx[0]]\n",
    "        pre_sents.append([\\\n",
    "                         sents[t-1] if (t-1)>=0 else '',\\\n",
    "                          sents[t-2] if (t-2)>=0 else '',\\\n",
    "                          sents[t-3] if (t-3)>=0 else ''\\\n",
    "                         ])\n",
    "    return pre_sents\n",
    "\n",
    "def get_current_utterance(mentions_idx,ut,ut_idx):\n",
    "    curr_ut = []\n",
    "    for idx in mentions_idx:\n",
    "        curr_ut.append(ut[ut_idx[idx[0]]])\n",
    "    return curr_ut\n",
    "\n",
    "def get_next_utterances(mentions_idx,ut,ut_idx):\n",
    "    next_ut = []\n",
    "    for idx in mentions_idx:\n",
    "        t = ut_idx[idx[-1]]+1\n",
    "        if t<len(ut):\n",
    "            next_ut.append(ut[t])\n",
    "    return next_ut\n",
    "\n",
    "def get_pre_utterances(mentions_idx,ut,ut_idx):\n",
    "    pre_ut = []\n",
    "    for idx in mentions_idx:\n",
    "        t = ut_idx[idx[0]]\n",
    "        pre_ut.append([\\\n",
    "                         ut[t-1] if (t-1)>=0 else '',\\\n",
    "                          ut[t-2] if (t-2)>=0 else '',\\\n",
    "                          ut[t-3] if (t-3)>=0 else ''\\\n",
    "                         ])\n",
    "    return pre_ut\n",
    "\n",
    "def get_current_speakers(mentions_idx,sents,sents_idx):\n",
    "    curr_speakers = []\n",
    "    for idx in mentions_idx:\n",
    "        curr_speakers.append(speakers[speakers_idx[idx[0]]])\n",
    "    return curr_speakers\n",
    "\n",
    "def get_pre_speakers(mentions_idx,speakers,speakers_idx):\n",
    "    print(\"new\")\n",
    "    pre_speakers = []\n",
    "    for idx in mentions_idx:\n",
    "        pre_speakers.append([\\\n",
    "                           speakers[speakers_idx[idx[0]]-1] if (speakers_idx[idx[0]]-1)>=0 else '',\\\n",
    "                           speakers[speakers_idx[idx[0]]-2] if (speakers_idx[idx[0]]-2)>=0 else ''\\\n",
    "                          ])\n",
    "    return pre_speakers\n",
    "\n",
    "def get_embeddings(l):\n",
    "    #ip : array of words : n\n",
    "    #op : array : embedding of each word : n\n",
    "    return np.array([embeddings_model[i] for i in l])\n",
    "\n",
    "def get_avg_embeddings(l):\n",
    "    #ip : array of sentences : n x s\n",
    "    #op : array : average of embeddings of all words for each sentence : n\n",
    "    a = [np.array([embeddings_model[i] for i in sent.split()]) for sent in l]\n",
    "    return np.array([np.sum(i,axis=0)/len(i) for i in a])\n",
    "\n",
    "def get_multiple_avg_embeddings(l):\n",
    "    #ip : array of array of sentences : n x 3 x s\n",
    "    #op : array of array of average of embeddings of all words for each sentence : n x 3\n",
    "    ret = []\n",
    "    for sent_arr in l:\n",
    "        sent_arr_emb = []\n",
    "        for sent in sent_arr:\n",
    "            if sent=='':\n",
    "                sent_arr_emb.append([0]*embeddings_size)\n",
    "            else:\n",
    "                sent_emb = []\n",
    "                for word in sent.split():\n",
    "                    sent_emb.append(embeddings_model[word])\n",
    "                sent_arr_emb.append(np.sum(sent_emb,axis=0)/len(sent_emb))\n",
    "        ret.append(sent_arr_emb)\n",
    "    return np.array(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fasttext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6a8c8fdad7ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m     \u001b[0m__main__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-6a8c8fdad7ce>\u001b[0m in \u001b[0;36m__main__\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m        \u001b[1;31m# f.write(make_transcript(dfs))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# embeddings_model = fasttext.skipgram(text_transcript,'embeddings_model',min_count=1,dim=50)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0membeddings_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'embeddings_model.bin'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Embeddings trained\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mpairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fasttext' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    __main__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new\n",
      "speakers done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train = pd.read_csv('data/friends.train.episode_delim.conll',sep='\\s+',header=None,comment='#')\n",
    "df = train\n",
    "mentions,mentions_y,mentions_idx = get_mention_arrays(df)\n",
    "print('Mentions done')\n",
    "words,words_idx = get_words_idx(df)\n",
    "pre_words = get_pre_words(mentions_idx,words,words_idx)\n",
    "next_words = get_next_words(mentions_idx,words,words_idx)\n",
    "print('words done')\n",
    "sents,sents_idx = get_sent_idx(df)\n",
    "curr_sents = get_current_sents(mentions_idx,sents,sents_idx)\n",
    "next_sents = get_next_sents(mentions_idx,sents,sents_idx)\n",
    "pre_sents = get_pre_sents(mentions_idx,sents,sents_idx)\n",
    "print('sents done')\n",
    "ut,ut_idx = get_utterances_idx(df)\n",
    "curr_ut = get_current_utterance(mentions_idx,ut,ut_idx)\n",
    "next_ut = get_next_utterances(mentions_idx,ut,ut_idx)\n",
    "pre_ut = get_pre_utterances(mentions_idx,ut,ut_idx)\n",
    "print('ut done')\n",
    "speakers,speakers_idx = get_speakers_idx(df)\n",
    "curr_speakers = get_current_speakers(mentions_idx,speakers,speakers_idx)\n",
    "pre_speakers = get_pre_speakers(mentions_idx,speakers,speakers_idx)\n",
    "print('speakers done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     32,       4,       3,      23,     234,     234, 2133213,\n",
       "           123,    2324,      34,      23])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([[32,4,3,23,234,234],[2133213],[123,2324,34,23]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mentions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-47be780afde0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mmention_pair_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmentions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurr_speakers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurr_sents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'mentions' is not defined"
     ]
    }
   ],
   "source": [
    "def mention_pair_features(mentions,speakers,sentences,window=7):\n",
    "    def exact_string_match(m1,m2):\n",
    "        return int(m1==m2)\n",
    "    def speaker_match(s1,s2):\n",
    "        return int(s1==s2)\n",
    "    def edit_distance(m1,m2):\n",
    "        return stringdist.levenshtein(m1,m2)\n",
    "    tuples = []\n",
    "    for i in range(len(mentions)):\n",
    "        for j in range(i+1,window+i+1):\n",
    "            if j<len(mentions):\n",
    "                tuples.append([exact_string_match(mentions[i],mentions[j]),\\\n",
    "                               speaker_match(speakers[i],speakers[j]),\\\n",
    "                               edit_distance(mentions[i],mentions[j]),\\\n",
    "                               edit_distance(sentences[i],sentences[j])])\n",
    "            else:\n",
    "                break\n",
    "    return np.array(tuples)\n",
    "\n",
    "mention_pair_features(mentions[:10],curr_speakers[:10],curr_sents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(m1,m2):\n",
    "    return int(m1==m2)\n",
    "def f2(m1,m2):\n",
    "    return int(m1==m2)\n",
    "def f3(s1,s2):\n",
    "    return int(s1==s2)\n",
    "def f4(m1,m2):\n",
    "    return stringdist.levenshtein(m1,m2)\n",
    "def f5(s1,s2):\n",
    "    return stringdist.levenshtein(s1,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f3(mentions[0],mentions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
